# ğŸ›¡ï¸ Incident Grade Prediction Pipeline  
A complete end-to-end machine learning project for **Security Incident Grade Classification** using:

- **EDA** (Exploratory Data Analysis)
- **Feature Engineering** (Frequency Encoding + OHE + Time Features)
- **Model Training** (GPU-accelerated XGBoost)
- **Streamlit Web App** for evaluations & interactive predictions  
- **Reusable ML artifacts** stored via Joblib

All major work is contained in a single notebook:  
ğŸ“˜ **`model.ipynb`**

---

## ğŸ“Š 1. Project Overview  

This project predicts incident-grade labels for security alert logs:

- `FalsePositive`
- `BenignPositive`
- `TruePositive`

Using the official GUIDE dataset from Kaggle.

ğŸ¯ **Final Model Performance on Validation Data**:

| Metric | Score |
|-------|-------|
| **Accuracy** | **0.8717** |
| **Quadratic Kappa (QWK)** | **0.8412** |
| **Best Iteration** | 334 trees |

Confusion Matrix:

```
[[1665510   46899   40531]
 [ 146358  622431  133909]
 [ 133533   30874 1327947]]
```

---

## ğŸ“‚ 2. Repository Structure  

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_link.txt        # Kaggle link for dataset download
â”‚   â”œâ”€â”€ GUIDE_Train.csv      # (Place after download)
â”‚   â”œâ”€â”€ GUIDE_Test.csv       # (Place after download)
â”‚
â”œâ”€â”€ joblibs/                 # Saved ML artifacts
â”‚   â”œâ”€â”€ frequency_maps.joblib
â”‚   â”œâ”€â”€ target_label_encoder.joblib
â”‚   â”œâ”€â”€ training_columns.joblib
â”‚   â”œâ”€â”€ xgboost_incident_grade_model.joblib
â”‚
â”œâ”€â”€ app.py                   # Streamlit App for prediction & demo
â”œâ”€â”€ model.ipynb              # Main notebook â†’ EDA + Feature Engineering + Training Pipeline
â”œâ”€â”€ README.md                # (this file)
```

---

## ğŸ“¥ 3. Download Dataset from Kaggle  

Inside:

```
./data/data_link.txt
```

You will find the Kaggle URL for the full dataset.

### **Steps:**

1. Open the link in `data_link.txt`
2. Download:
   - `GUIDE_Train.csv`
   - `GUIDE_Test.csv`
3. Place both inside:

```
./data/
```

---

## âš™ï¸ 4. Setup & Installation  

No virtual environment is required, but recommended.

### Install Python (3.9+)
Check your version:

```bash
python --version
```

---

## ğŸ“¦ 5. Install Required Libraries  

Run:

```bash
pip install pandas numpy plotly scikit-learn joblib xgboost streamlit matplotlib seaborn fastparquet pyarrow
```

### GPU Acceleration  
If CUDA is installed, XGBoost automatically uses:

```
device='cuda'
```

Otherwise it falls back to CPU.

---

## â–¶ï¸ 6. Running the Notebook (`model.ipynb`)  

Open JupyterLab or VSCode and run:

```bash
jupyter notebook model.ipynb
```

Inside the notebook you will find:

### âœ” Full EDA  
- Target distribution  
- Missing value analysis  
- Cardinality reports  
- Visualizations (Pie chart, Null bars, etc.)

### âœ” Feature Engineering  
- Timestamp â†’ Hour, DayOfWeek, Month  
- IsBusinessHours  
- Frequency Encoding (train-based maps saved to joblibs/)  
- One-Hot Encoding  
- Column alignment  

### âœ” Model Training  
- XGBoost (multi:softprob) with early stopping  
- Evaluation on clean validation  
- Saving artifacts to `joblibs/`

### âœ” Exported Artifacts  
Your notebook saves:

```
frequency_maps.joblib
target_label_encoder.joblib
training_columns.joblib
xgboost_incident_grade_model.joblib
```

---

## ğŸŒ 7. Run the Streamlit Demo App  

Your `app.py` loads:

- The trained XGBoost model
- All joblib artifacts
- Validation dataset
- Feature pipeline  
- Produces predictions + metrics + interactive confusion matrix

### Launch:

```bash
streamlit run app.py
```

Then open:

```
http://localhost:8501
```

---

## ğŸ§  8. Feature Engineering Summary  

### Temporal Features  
| Feature | Description |
|---------|-------------|
| Hour | 0â€“23 |
| DayOfWeek | 0â€“6 |
| Month | 1â€“12 |
| IsBusinessHours | 1 if 8AMâ€“6PM |

### Frequency Encoding  
Applied to all high-cardinality categorical columns.

### One-Hot Encoding  
Applied to:

```
Hour  
DayOfWeek  
Month  
IsBusinessHours  
Category  
OSFamily  
OSVersion  
EntityType  
EvidenceRole
```

### Label Encoding  
```
FalsePositive â†’ 0  
BenignPositive â†’ 1  
TruePositive  â†’ 2
```

---

## ğŸ§ª 9. Model Details  

```python
xgb.XGBClassifier(
    objective='multi:softprob',
    num_class=3,
    n_estimators=500,
    device='cuda',
    random_state=42,
    eval_metric='mlogloss',
    early_stopping_rounds=50
)
```

Best iteration: **334**

---

## ğŸš€ 10. Workflow  

1. Download Kaggle data  
2. Run `model.ipynb`  
3. Verify joblibs are created  
4. Launch Streamlit app  
5. Interactively inspect predictions  

